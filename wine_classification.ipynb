{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the dataset(s) for your project. If a train set and a test set are not already available, randomly split the dataset into a train and a test set using stratified sampling so that 80% of the samples go to train set and 20% to test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"wine.csv\")\n",
    "y = data['type']\n",
    "X = data.loc[:, data.columns != 'type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Randomly split your train set into a validation and a new train set (called train set 2) such that the validation set contains 1/5 of the samples in original train set and the train set 2 contains the remaining. Use stratified sampling to assign features to train set 2 and validation set. This should ensure that your validation set contains samples from both classes (i.e. ciliary and non-ciliary with equal proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_two, X_validation, y_train_two, y_validation = train_test_split(X_train, y_train, \n",
    "                                                                        test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Normalize features in your train set 2 and validation set using min-max scaling to interval [0,1]. For this purpose you can first normalize features in your train set 2 and use the same scaling coefficients to normalize validation set. Save the normalized versions as separate files. Repeat normalizing your original train set and use the same normalization coefficients to normalize the two test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muhammet/.local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train_two)\n",
    "normalized_x_train_two = scaler.transform(X_train_two)\n",
    "normalized_x_validation = scaler.transform(X_validation)\n",
    "\n",
    "np.savetxt(\"normalized_x_train_two.csv\", normalized_x_train_two, delimiter=\",\")\n",
    "np.savetxt(\"normalized_x_validation.csv\", normalized_x_validation, delimiter=\",\")\n",
    "\n",
    "scaler.fit(normalized_x_train_two)\n",
    "normalized_x_train = scaler.transform(X_train)\n",
    "normalized_x_test = scaler.transform(X_test)\n",
    "normalized_x_validation_with_orig = scaler.transform(X_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Perform a 10-fold cross-validation experiment for the random forest classifier on normalized and unnormalized versions of train set 2. You can set the number of trees to 100. Do you get better accuracy when you perform data normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, I did not. Unnormalized Accuracy: 0.931034482759, Normalized Accuracy: 0.931034482759\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_two, y_train_two)\n",
    "\n",
    "unnormalized_accuracy = clf.score(X_validation, y_validation)\n",
    "\n",
    "clf.fit(normalized_x_train_two, y_train_two)\n",
    "normalized_accuracy = clf.score(normalized_x_validation, y_validation)\n",
    "\n",
    "if unnormalized_accuracy > normalized_accuracy or unnormalized_accuracy == normalized_accuracy:\n",
    "    print(\"No, I did not. Unnormalized Accuracy: {}, Normalized Accuracy: {}\"\n",
    "          .format(unnormalized_accuracy,normalized_accuracy))\n",
    "else:\n",
    "    print(\"Yes, I did. Unnormalized Accuracy: {}, Normalized Accuracy: {}\"\n",
    "          .format(unnormalized_accuracy,normalized_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Perform a 10-fold cross-validation experiment on train set 2 that corresponds to the best performing normalization strategy (i.e. normalized or unnormalized) for the following classifiers: \n",
    "\n",
    "Logistic regression\n",
    "\n",
    "k-nearest neighbor (with k=1)\n",
    "\n",
    "Na√Øve Bayes\n",
    "\n",
    "Decision tree\n",
    "\n",
    "Random forest (number of trees=100)\n",
    "\n",
    "SVM (RBF kernel C=1.0 gamma=0.125)\n",
    "\n",
    "RBF network (number of clusters = 3)\n",
    "\n",
    "Adaboost (number of iterations=10)\n",
    "\n",
    "You can use default values for other hyper-parameters of the classifiers\n",
    "Report the following accuracy measures for each of these classifiers: overall\n",
    "accuracy, F-measure, sensitivity, specificity, precision, area under the ROC curve,\n",
    "area under the precision recall curve, MCC scores. These will be cross-validation\n",
    "accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnormalized Results:\n",
      "\n",
      "Logistic Regression Accuracy Score: 0.955303030303\n",
      "K-Nearest Neighbour Accuracy Score: 0.671212121212\n",
      "Naive Bayes Accuracy Score: 0.94696969697\n",
      "Decision Tree Accuracy Score: 0.910606060606\n",
      "Random Forest Accuracy Score: 0.991666666667\n",
      "Support Vector Machine-rbf Accuracy Score: 0.418181818182\n",
      "AdaBoostClassifier Accuracy Score: 0.884090909091\n",
      "\n",
      "\n",
      "Logistic Regression F-Measure: 0.955193325193\n",
      "K-Nearest Neighbour F-Measure: 0.676984454939\n",
      "Naive Bayes F-Measure: 0.946775329048\n",
      "Decision Tree F-Measure: 0.897723017723\n",
      "Random Forest F-Measure: 0.991798941799\n",
      "Support Vector Machine-rbf F-Measure: 0.281352830176\n",
      "AdaBoostClassifier F-Measure: 0.87499775477\n",
      "\n",
      "\n",
      "Logistic Regression Sensitivity: 0.955303030303\n",
      "K-Nearest Neighbour Sensitivity: 0.671212121212\n",
      "Naive Bayes Sensitivity: 0.94696969697\n",
      "Decision Tree Sensitivity: 0.901515151515\n",
      "Random Forest Sensitivity: 0.982575757576\n",
      "Support Vector Machine-rbf Sensitivity: 0.418181818182\n",
      "AdaBoostClassifier Sensitivity: 0.884090909091\n",
      "\n",
      "\n",
      "Logistic Regression Precision: 0.967397186147\n",
      "K-Nearest Neighbour Precision: 0.733784271284\n",
      "Naive Bayes Precision: 0.956123737374\n",
      "Decision Tree Precision: 0.915643939394\n",
      "Random Forest Precision: 0.969431818182\n",
      "Support Vector Machine-rbf Precision: 0.246416437098\n",
      "AdaBoostClassifier Precision: 0.894116161616\n",
      "\n",
      "\n",
      "Logistic Regression MCC: 0.929294085394\n",
      "K-Nearest Neighbour MCC: 0.497145198364\n",
      "Naive Bayes MCC: 0.913586636863\n",
      "Decision Tree MCC: 0.815745115994\n",
      "Random Forest MCC: 0.942415567474\n",
      "Support Vector Machine-rbf MCC: 0.0664475642813\n",
      "AdaBoostClassifier MCC: 0.814572656309\n",
      "\n",
      "\n",
      "Normalized Results:\n",
      "\n",
      "Logistic Regression Accuracy Score: 0.955303030303\n",
      "K-Nearest Neighbour Accuracy Score: 0.937878787879\n",
      "Naive Bayes Accuracy Score: 0.94696969697\n",
      "Decision Tree Accuracy Score: 0.920454545455\n",
      "Random Forest Accuracy Score: 0.973484848485\n",
      "Support Vector Machine-rbf Accuracy Score: 0.973484848485\n",
      "AdaBoostClassifier Accuracy Score: 0.884090909091\n",
      "\n",
      "\n",
      "Logistic Regression F-Measure: 0.956840264113\n",
      "K-Nearest Neighbour F-Measure: 0.937977279455\n",
      "Naive Bayes F-Measure: 0.946775329048\n",
      "Decision Tree F-Measure: 0.890075059621\n",
      "Random Forest F-Measure: 0.970678647951\n",
      "Support Vector Machine-rbf F-Measure: 0.973617123617\n",
      "AdaBoostClassifier F-Measure: 0.87499775477\n",
      "\n",
      "\n",
      "Logistic Regression Sensitivity: 0.955303030303\n",
      "K-Nearest Neighbour Sensitivity: 0.937878787879\n",
      "Naive Bayes Sensitivity: 0.94696969697\n",
      "Decision Tree Sensitivity: 0.911363636364\n",
      "Random Forest Sensitivity: 0.982575757576\n",
      "Support Vector Machine-rbf Sensitivity: 0.973484848485\n",
      "AdaBoostClassifier Sensitivity: 0.884090909091\n",
      "\n",
      "\n",
      "Logistic Regression Precision: 0.975189393939\n",
      "K-Nearest Neighbour Precision: 0.956563852814\n",
      "Naive Bayes Precision: 0.956123737374\n",
      "Decision Tree Precision: 0.899886363636\n",
      "Random Forest Precision: 0.977386363636\n",
      "Support Vector Machine-rbf Precision: 0.975568181818\n",
      "AdaBoostClassifier Precision: 0.894116161616\n",
      "\n",
      "\n",
      "Logistic Regression MCC: 0.932191341739\n",
      "K-Nearest Neighbour MCC: 0.908552678595\n",
      "Naive Bayes MCC: 0.913586636863\n",
      "Decision Tree MCC: 0.844088435086\n",
      "Random Forest MCC: 0.971630981146\n",
      "Support Vector Machine-rbf MCC: 0.956073010536\n",
      "AdaBoostClassifier MCC: 0.814572656309\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "models.append((\"Logistic Regression\",LogisticRegression()))\n",
    "models.append((\"K-Nearest Neighbour\",KNeighborsClassifier(n_neighbors=1)))\n",
    "models.append((\"Naive Bayes\",GaussianNB()))\n",
    "models.append((\"Decision Tree\",DecisionTreeClassifier()))\n",
    "models.append((\"Random Forest\",RandomForestClassifier(n_estimators=100)))\n",
    "models.append((\"Support Vector Machine-rbf\",SVC(kernel=\"rbf\", C=1,gamma=0.125)))\n",
    "#models.append((\"RBF Network:\", RBFNetwork()))\n",
    "models.append((\"AdaBoostClassifier\",AdaBoostClassifier()))\n",
    "\n",
    "metrics = []\n",
    "\n",
    "metrics.append((\"Accuracy Score\", accuracy_score, \"\"))\n",
    "metrics.append((\"F-Measure\", f1_score, \"weighted\"))\n",
    "metrics.append((\"Sensitivity\", recall_score, \"weighted\"))\n",
    "#metrics.append((\"Specificity\", recall_score, \"weighted\"))\n",
    "metrics.append((\"Precision\", precision_score, \"weighted\"))\n",
    "#metrics.append((\"Area Under ROC Curve\", roc_auc_score, \"weighted\"))\n",
    "#metrics.append((\"Area Under Precision Recall Curve\", precision_recall_curve, \"weighted\"))\n",
    "metrics.append((\"MCC\", matthews_corrcoef, \"\"))\n",
    "\n",
    "def dump_results(X, y):\n",
    "    \n",
    "    for metric_name, metric, avg in metrics:\n",
    "        names = []\n",
    "        results = []\n",
    "        \n",
    "        for name,model in models:\n",
    "            kfold = KFold(n_splits=10, random_state=42)\n",
    "            if metric_name == \"Accuracy Score\" or metric_name == \"MCC\":\n",
    "                scorer = make_scorer(metric)\n",
    "            else:\n",
    "                scorer = make_scorer(metric, average=avg)\n",
    "            \n",
    "            cv_result = cross_val_score(model,X,y.values.ravel(), cv = kfold,scoring = scorer)\n",
    "            names.append(name)\n",
    "            results.append(cv_result)\n",
    "\n",
    "        for i in range(len(names)):\n",
    "            print(\"{} {}: {}\".format(names[i], metric_name, results[i].mean()))\n",
    "        \n",
    "        print(\"\\n\")\n",
    "\n",
    "print(\"Unnormalized Results:\\n\")\n",
    "dump_results(X_train_two, y_train_two)\n",
    "\n",
    "print(\"Normalized Results:\\n\")\n",
    "dump_results(normalized_x_train_two, y_train_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use three feature selection methods to select feature subsets on train set 2 and compute accuracy measures in step 5 for all the classifiers. Repeat for normalized version of train set 2. Do you get improvement in accuracy when you perform feature selection or is it better to use all of the features? Which feature selection strategy gives the best accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
