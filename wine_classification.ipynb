{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the dataset(s) for your project. If a train set and a test set are not already available, randomly split the dataset into a train and a test set using stratified sampling so that 80% of the samples go to train set and 20% to test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"wine.csv\")\n",
    "y = data['type']\n",
    "X = data.loc[:, data.columns != 'type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Randomly split your train set into a validation and a new train set (called train set 2) such that the validation set contains 1/5 of the samples in original train set and the train set 2 contains the remaining. Use stratified sampling to assign features to train set 2 and validation set. This should ensure that your validation set contains samples from both classes (i.e. ciliary and non-ciliary with equal proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_two, X_validation, y_train_two, y_validation = train_test_split(X_train, y_train, \n",
    "                                                                        test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Normalize features in your train set 2 and validation set using min-max scaling to interval [0,1]. For this purpose you can first normalize features in your train set 2 and use the same scaling coefficients to normalize validation set. Save the normalized versions as separate files. Repeat normalizing your original train set and use the same normalization coefficients to normalize the two test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train_two)\n",
    "normalized_x_train_two = scaler.transform(X_train_two)\n",
    "normalized_x_validation = scaler.transform(X_validation)\n",
    "\n",
    "np.savetxt(\"normalized_x_train_two.csv\", normalized_x_train_two, delimiter=\",\")\n",
    "np.savetxt(\"normalized_x_validation.csv\", normalized_x_validation, delimiter=\",\")\n",
    "\n",
    "scaler.fit(normalized_x_train_two)\n",
    "normalized_x_train = scaler.transform(X_train)\n",
    "normalized_x_test = scaler.transform(X_test)\n",
    "normalized_x_validation_with_orig = scaler.transform(X_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Perform a 10-fold cross-validation experiment for the random forest classifier on normalized and unnormalized versions of train set 2. You can set the number of trees to 100. Do you get better accuracy when you perform data normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, I did not. Unnormalized Accuracy: 0.931034482759, Normalized Accuracy: 0.931034482759\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_two, y_train_two)\n",
    "\n",
    "unnormalized_accuracy = clf.score(X_validation, y_validation)\n",
    "\n",
    "clf.fit(normalized_x_train_two, y_train_two)\n",
    "normalized_accuracy = clf.score(normalized_x_validation, y_validation)\n",
    "\n",
    "if unnormalized_accuracy > normalized_accuracy or unnormalized_accuracy == normalized_accuracy:\n",
    "    print(\"No, I did not. Unnormalized Accuracy: {}, Normalized Accuracy: {}\"\n",
    "          .format(unnormalized_accuracy,normalized_accuracy))\n",
    "else:\n",
    "    print(\"Yes, I did. Unnormalized Accuracy: {}, Normalized Accuracy: {}\"\n",
    "          .format(unnormalized_accuracy,normalized_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Perform a 10-fold cross-validation experiment on train set 2 that corresponds to the best performing normalization strategy (i.e. normalized or unnormalized) for the following classifiers: \n",
    "\n",
    "Logistic regression\n",
    "\n",
    "k-nearest neighbor (with k=1)\n",
    "\n",
    "Na√Øve Bayes\n",
    "\n",
    "Decision tree\n",
    "\n",
    "Random forest (number of trees=100)\n",
    "\n",
    "SVM (RBF kernel C=1.0 gamma=0.125)\n",
    "\n",
    "RBF network (number of clusters = 3)\n",
    "\n",
    "Adaboost (number of iterations=10)\n",
    "\n",
    "You can use default values for other hyper-parameters of the classifiers\n",
    "Report the following accuracy measures for each of these classifiers: overall\n",
    "accuracy, F-measure, sensitivity, specificity, precision, area under the ROC curve,\n",
    "area under the precision recall curve, MCC scores. These will be cross-validation\n",
    "accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnormalized Results:\n",
      "\n",
      "Logistic Regression Accuracy: 0.955303030303\n",
      "K-Nearest Neighbour Accuracy: 0.671212121212\n",
      "Naive Bayes Accuracy: 0.94696969697\n",
      "Decision Tree Accuracy: 0.910606060606\n",
      "Random Forest Accuracy: 0.991666666667\n",
      "Support Vector Machine-rbf Accuracy: 0.418181818182\n",
      "AdaBoostClassifier Accuracy: 0.884090909091\n",
      "\n",
      "Normalized Results:\n",
      "\n",
      "Logistic Regression Accuracy: 0.955303030303\n",
      "K-Nearest Neighbour Accuracy: 0.937878787879\n",
      "Naive Bayes Accuracy: 0.94696969697\n",
      "Decision Tree Accuracy: 0.883333333333\n",
      "Random Forest Accuracy: 0.982575757576\n",
      "Support Vector Machine-rbf Accuracy: 0.973484848485\n",
      "AdaBoostClassifier Accuracy: 0.884090909091\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "models.append((\"Logistic Regression\",LogisticRegression()))\n",
    "models.append((\"K-Nearest Neighbour\",KNeighborsClassifier(n_neighbors=1)))\n",
    "models.append((\"Naive Bayes\",GaussianNB()))\n",
    "models.append((\"Decision Tree\",DecisionTreeClassifier()))\n",
    "models.append((\"Random Forest\",RandomForestClassifier(n_estimators=100)))\n",
    "models.append((\"Support Vector Machine-rbf\",SVC(kernel=\"rbf\", C=1,gamma=0.125)))\n",
    "#models.append((\"RBF Network:\", RBFNetwork()))\n",
    "models.append((\"AdaBoostClassifier\",AdaBoostClassifier()))\n",
    "\n",
    "measures = [\"accuracy\",\"f1\",\"recall\",\"precision\",\"roc_auc\"]\n",
    "\n",
    "unnormalized_results = []\n",
    "names = []\n",
    "for name,model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=42)\n",
    "    cv_result = cross_val_score(model,X_train_two,y_train_two.values.ravel(), cv = kfold,scoring = \"accuracy\")\n",
    "    names.append(name)\n",
    "    unnormalized_results.append(cv_result)\n",
    "\n",
    "print(\"Unnormalized Results:\\n\")\n",
    "\n",
    "for i in range(len(names)):\n",
    "    print(\"{} Accuracy: {}\".format(names[i],unnormalized_results[i].mean()))\n",
    "    \n",
    "normalized_results = []\n",
    "for name,model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=42)\n",
    "    cv_result = cross_val_score(model,normalized_x_train_two,y_train_two.values.ravel(), cv = kfold,scoring = \"accuracy\")\n",
    "    normalized_results.append(cv_result)\n",
    "    \n",
    "print(\"\\nNormalized Results:\\n\")\n",
    "\n",
    "for i in range(len(names)):\n",
    "    print(\"{} Accuracy: {}\".format(names[i],normalized_results[i].mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
